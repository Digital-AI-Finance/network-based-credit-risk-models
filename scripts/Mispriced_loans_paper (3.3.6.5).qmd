---
title: "Mispriced_loans_paper: Network Construction for Bondora"
format: pdf
editor: visual
---

# Mispriced_loans_paper: Interest rate modeling and Refined Pricing Analysis.

## Libraries and Environment

```{r}
# rm(list = ls())
```

```{r}
load_or_install_packages <- function(package_names) {
  # R
  #
  # 
  # - package_names (vector):
  #
  #
  # 
  #
  # 
  # load_or_install_packages(c('ggplot2', 'dplyr', 'tidyr'))

  failed_packages <- c()

  for (package_name in package_names) {
    if (!requireNamespace(package_name, quietly = TRUE)) {
      message(paste0("Attempting to install ", package_name, " package from CRAN..."))
      tryCatch({
        install.packages(package_name, repos = 'http://cran.rstudio.com/')
        library(package_name, character.only = TRUE)
        message(paste0(package_name, " package has been successfully installed and loaded."))
      }, error = function(e) {
        message(paste0("Failed to install ", package_name, ": ", e$message))
        failed_packages <- c(failed_packages, package_name)
      })
    } else {
      library(package_name, character.only = TRUE)
      message(paste0(package_name, " package is already installed and has been loaded."))
    }
  }

  if (length(failed_packages) > 0) {
    stop(paste0("Failed to install the following packages: ", paste(failed_packages, collapse = ", ")))
  }
}

```

```{r}
package_list <- c("dplyr", "ggplot2", "caret", "pROC", "randomForest",
    "e1071", "ROCR", "igraph", "scales", "tidytext", "Matrix", "RSpectra", "sandwich", "lmtest", "Metrics", "glmnet", "broom", "survminer", "RColorBrewer", "viridis")
load_or_install_packages(package_list)
```

## Functions

### RF Model for feature selection on predicting Interest (Regression Task)

```{r}
# Function to select top features for a regression task using Random Forest
select_top_features_rf_regression <- function(df_sample, 
                                              target_var,
                                              exclude_vars = NULL,
                                              top_n = 20) {
  # -------------------------------
  # 1) Prepare data
  # -------------------------------
  # Exclude the target variable + any user-specified variables
  if (!is.null(exclude_vars)) {
    predictor_vars <- setdiff(names(df_sample), c(target_var, exclude_vars))
  } else {
    predictor_vars <- setdiff(names(df_sample), target_var)
  }
  
  # Ensure the target variable is numeric
  # If 'Interest' is already numeric, this is harmless
  df_sample[[target_var]] <- as.numeric(df_sample[[target_var]])
  
  # -------------------------------
  # 2) Split data into training/testing sets
  # -------------------------------
  set.seed(123)  # for reproducibility
  train_index <- createDataPartition(df_sample[[target_var]], p = 0.7, list = FALSE)
  train_data <- df_sample[train_index, ]
  test_data  <- df_sample[-train_index, ]
  
  # -------------------------------
  # 3) Train Random Forest (Regression Mode)
  # -------------------------------
  # Note: type=1 in importance => "Increase in MSE" measure
  rf_model <- randomForest(
    as.formula(paste(target_var, "~ .")),
    data       = train_data[, c(target_var, predictor_vars), drop = FALSE],
    importance = TRUE,
    ntree      = 500
  )
  
  # -------------------------------
  # 4) Extract Variable Importance
  # -------------------------------
  # For regression, type=1 => %IncMSE
  importance_scores <- importance(rf_model, type = 1)
  importance_df <- data.frame(
    Feature    = rownames(importance_scores),
    Importance = importance_scores[, 1]
  )
  # Sort by descending importance
  importance_df <- importance_df[order(-importance_df$Importance), ]
  
  # -------------------------------
  # 5) Select Top N Features
  # -------------------------------
  top_features <- importance_df$Feature[1:top_n]
  
  # Return both top features and entire importance dataframe
  return(
    list(
      top_features      = top_features,
      rf_importance_df  = importance_df,
      rf_model          = rf_model  # optional: return the model if you like
    )
  )
}

```

```{r}
# Define the target variable as Interest (direct regression task)
target_var <- "Interest"
```

### Exclude Ex-Post Variables

```{r}
# List of variables to exclude
exclude_vars <- c("default", "Bondora_Rating", "LoanId", "Mispricing", "Delta", "Credit_Rating", "InterestRecovery", "PrincipalRecovery", "date.start", "date.end", "DefaultDate", "Interest", "ExpectedReturn", "Maturity", "log_loan_amount", "issue_year")
```

```{r}
# Run the feature selection
rf_reg_result <- select_top_features_rf_regression(
  df_sample    = df_sample,
  target_var   = "Interest",   # We want to predict 'Interest'
  exclude_vars = exclude_vars,
  top_n        = 20
)

# Check the selected top 20 features
rf_reg_result$top_features

```

### Define Predictor Variables

```{r}
# Define predictor variables (exclude target and ex-post variables)
predictor_vars <- rf_reg_result$top_features
```

```{r}
print(predictor_vars)
```

## Fit GBM (cross-validated) for each Community and Evaluating Model Performance

```{r}

###################################
# 1) GBM Hyperparameter Tuning
###################################
# We'll define a grid for caret's "gbm" method.
# Typically includes interaction.depth, n.trees, shrinkage, and n.minobsinnode.
# You can expand or modify as needed.

gbm_grid <- expand.grid(
  interaction.depth = c(3, 5),      # Tree depth
  n.trees           = c(100, 200),  # Number of boosting iterations
  shrinkage         = c(0.1, 0.3),  # Learning rate
  n.minobsinnode    = c(10, 20)     # Min # of obs in terminal node
)

train_ctrl <- trainControl(
  method = "repeatedcv",  # repeated cross-validation
  number = 5,            # 5-fold
  repeats = 1,
  verboseIter = FALSE
)

######################################################
# 2) Initialize storage lists & predictor setup
######################################################
community_models          <- list()
community_results         <- list()
mispricing_summary_list   <- list()

# We'll assume:
# - df_scaled has columns:
#    * community
#    * Mispricing (Overpriced / Properly Priced)
#    * Interest (target variable)
#    * plus your predictor columns
# - predictor_vars is a character vector of predictor column names

############################################################################
# 3) Loop through each community, Fit a GBM with caret + near‐zero-variance
############################################################################
for (comm in unique(df_sample$community)) {
  cat("\nProcessing Community:", comm, "\n")
  
  # Subset data for the current community
  df_comm <- df_sample %>% filter(community == comm)
  
  # Subset properly priced loans
  properly_priced_comm <- df_comm %>% filter(Mispricing == "Properly Priced")
  
  # Check for sufficient data
  if (nrow(properly_priced_comm) < 30) {
    cat("Not enough properly priced loans to train model for Community", comm, "\n")
    next
  }
  
  #---------------------------------------------
  # Step A: Partition into Train (70%) & Validation (30%)
  #---------------------------------------------
  set.seed(123)
  train_index <- createDataPartition(
    properly_priced_comm[["Interest"]],
    p = 0.7,
    list = FALSE
  )
  
  train_data <- properly_priced_comm[train_index, ]
  validation_data <- properly_priced_comm[-train_index, ]
  
  #---------------------------------------------
  # Step B: Remove single-level variables from the training set
  #---------------------------------------------
  # caretaker *also* will remove near‐zero-variance columns if we specify
  # preProcess="zv". But removing them upfront at the train_data level can help
  # keep your dataset consistent across downstream steps.
  
  unique_value_counts <- sapply(
    train_data[, predictor_vars, drop = FALSE],
    function(x) length(unique(x))
  )
  single_level_vars <- names(unique_value_counts[unique_value_counts <= 1])
  
  if (length(single_level_vars) > 0) {
    cat("Single-level vars for Community", comm, "in TRAIN data:",
        paste(single_level_vars, collapse = ", "), "\n")
  }
  
  # Exclude single-level from both train & validation
  predictor_vars_comm <- setdiff(predictor_vars, single_level_vars)
  
  if (length(predictor_vars_comm) == 0) {
    cat("No valid predictors left in TRAIN data for Community", comm, ". Skipping.\n")
    next
  }
  
  train_data_sub <- train_data[, c(predictor_vars_comm, "Interest"), drop = FALSE]
  validation_data_sub <- validation_data[, c(predictor_vars_comm, "Interest"), drop = FALSE]
  
  #---------------------------------------------
  # Step C: Create formula & train the GBM
  #        Using caretaker's near-zero-variance removal
  #---------------------------------------------
  formula_str <- paste("Interest", "~", paste(predictor_vars_comm, collapse = " + "))
  cat("Caret GBM formula for Community", comm, ":\n", formula_str, "\n")
  
  set.seed(123)
  gbm_model_caret <- train(
    form      = as.formula(formula_str),
    data      = train_data_sub,
    method    = "gbm",
    trControl = train_ctrl,
    tuneGrid  = gbm_grid,
    metric    = "RMSE",
    verbose   = FALSE,           # suppress extra gbm output
    preProcess = "zv"            # <--- near-zero-variance removal inside CV folds
    # If caretaker's gbm should incorporate row-level weights:
    # weights = train_data_sub[["loan_amount"]]
  )
  
  # Extract best model
  best_gbm_model <- gbm_model_caret$finalModel
  
  #---------------------------------------------
  # Step D: Validate on the 30% hold-out
  #---------------------------------------------
  val_predictions <- predict(gbm_model_caret, newdata = validation_data_sub)
  
  rmse_val <- sqrt(mean((validation_data_sub[["Interest"]] - val_predictions)^2))
  mae_val  <- mean(abs(validation_data_sub[["Interest"]] - val_predictions))
  
  ss_res <- sum((validation_data_sub[["Interest"]] - val_predictions)^2)
  ss_tot <- sum((validation_data_sub[["Interest"]] - mean(validation_data_sub[["Interest"]]))^2)
  r_squared_val <- 1 - ss_res / ss_tot
  
  cat("Validation RMSE:", rmse_val, "\n")
  cat("Validation MAE:", mae_val, "\n")
  cat("Validation R-squared:", r_squared_val, "\n")
  cat("Best Tune:\n")
  print(gbm_model_caret$bestTune)
  cat("\n")
  
  #---------------------------------------------
  # Step E: Predict on *all* properly priced to find threshold
  #---------------------------------------------
  # Must also drop single-level vars from properly_priced_comm before predict
  # since caretaker won't accept columns that differ from the training set
  proper_subset <- properly_priced_comm[, c(predictor_vars_comm, "Interest"), drop=FALSE]
  
  proper_preds <- predict(gbm_model_caret, newdata = proper_subset)
  
  properly_priced_comm$Predicted_Interest  <- proper_preds
  properly_priced_comm$Interest_Difference <- properly_priced_comm$Predicted_Interest - properly_priced_comm$Interest
  
  pos_diff <- properly_priced_comm$Interest_Difference[
               properly_priced_comm$Interest_Difference > 0
             ]
  threshold_underpriced <- mean(pos_diff, na.rm = TRUE) + sd(pos_diff, na.rm = TRUE)
  
  neg_diff <- properly_priced_comm$Interest_Difference[
               properly_priced_comm$Interest_Difference < 0
             ]
  threshold_overpriced <- mean(neg_diff, na.rm = TRUE) - sd(neg_diff, na.rm = TRUE)
  
  #---------------------------------------------
  # Step F: Predict on mispriced loans
  #---------------------------------------------
  mispriced_comm <- df_comm %>% filter(Mispricing %in% c("Overpriced", "Underpriced"))
  if (nrow(mispriced_comm) > 0) {
    # also remove single-level from mispriced subset
    mispriced_subset <- mispriced_comm[, c(predictor_vars_comm, "Interest"), drop=FALSE]
    
    test_preds <- predict(gbm_model_caret, newdata = mispriced_subset)
    mispriced_comm$Predicted_Interest  <- test_preds
    mispriced_comm$Interest_Difference <- mispriced_comm$Predicted_Interest - mispriced_comm$Interest
    
    mispriced_comm <- mispriced_comm %>%
      mutate(
        Mispricing_Type = case_when(
          Interest_Difference > threshold_underpriced ~ "Underpriced",
          Interest_Difference < threshold_overpriced  ~ "Overpriced",
          TRUE ~ "Properly Priced"
        )
      )
    
    # Store in community_results
    community_results[[as.character(comm)]] <- mispriced_comm
    
    # Summarize
    mispricing_summary <- mispriced_comm %>%
      group_by(Mispricing_Type) %>%
      summarise(
        Count                      = n(),
        Mean_Actual_Interest       = mean(Interest, na.rm = TRUE),
        Mean_Predicted_Interest    = mean(Predicted_Interest, na.rm = TRUE),
        Mean_Interest_Difference   = mean(Interest_Difference, na.rm = TRUE),
        Median_Interest_Difference = median(Interest_Difference, na.rm = TRUE),
        .groups = "drop"
      ) %>%
      mutate(Community = comm)
    
    mispricing_summary_list[[as.character(comm)]] <- mispricing_summary
  } else {
    cat("No mispriced loans to predict in Community", comm, "\n")
  }
  
  # Save caretaker object if you like
  community_models[[as.character(comm)]] <- gbm_model_caret
}

cat("All communities processed.\n")
```

```{r}
# Combine mispricing summaries from all communities
mispricing_summary <- bind_rows(mispricing_summary_list)

# View the combined mispricing summary
print(mispricing_summary)
```

```{r}
# Combine mispriced loans from all communities
all_mispriced_loans <- bind_rows(community_results)

# View the combined mispriced loans data
head(all_mispriced_loans)

```

```{r}
# Overwrite 'Mispricing' in df_sample with 'Mispricing_Type' from all_mispriced_loans
df_sample_updated <- df_sample %>%
  left_join(
    all_mispriced_loans %>%
      select(LoanId, Mispricing_Type),
    by = "LoanId"
  ) %>%
  mutate(
    # If Mispricing_Type exists for this LoanId, use it; otherwise keep the old Mispricing
    Mispricing = coalesce(Mispricing_Type, Mispricing)
  ) %>%
  select(-Mispricing_Type)  # Remove the temporary column

df_sample <- df_sample_updated
```

```{r}
# Update properly priced loan data frame
# Filter to include only properly priced loans
properly_priced_data <- df_sample %>%
  filter(Mispricing == "Properly Priced")
```

```{r}
# Economic Significance Comparision Tables

library(dplyr)

# 1) per‐community totals from the full df_sample
total_by_comm <- df_sample %>%
  group_by(community) %>%
  summarise(
    total_loans     = n(),
    total_principal = sum(loan_amount, na.rm = TRUE),
    avg_principal   = total_principal / total_loans
  )

# 2) per‐community mispricing stats—but now *filter* to only Over/Under‑priced
mis_by_comm <- all_mispriced_loans %>%
  filter(Mispricing_Type %in% c("Overpriced", "Underpriced")) %>%
  group_by(community) %>%
  summarise(
    mispriced_loans = n(),
    mis_1pp         = sum(abs(Interest_Difference) >= 1, na.rm = TRUE),
    mis_2pp         = sum(abs(Interest_Difference) >= 2, na.rm = TRUE),
    annual_transfer = sum(loan_amount * Interest_Difference/100, na.rm = TRUE)
  )

# 3) join and compute proportions (now mispriced_loans really only counts Over/Under)
community_econ_summary <- total_by_comm %>%
  left_join(mis_by_comm, by = "community") %>%
  replace_na(list(
    mispriced_loans = 0,
    mis_1pp         = 0,
    mis_2pp         = 0,
    annual_transfer = 0
  )) %>%
  mutate(
    prop_mispriced = mispriced_loans / total_loans,
    prop_1pp       = mis_1pp       / total_loans,
    prop_2pp       = mis_2pp       / total_loans
  )

print(community_econ_summary)

# 4) overall roll‑up
overall_econ_summary <- community_econ_summary %>%
  summarise(
    total_loans     = sum(total_loans),
    total_principal = sum(total_principal),
    avg_principal   = total_principal / total_loans,
    mispriced_loans = sum(mispriced_loans),
    prop_mispriced  = mispriced_loans / total_loans,
    mis_1pp         = sum(mis_1pp),
    prop_1pp        = mis_1pp    / total_loans,
    mis_2pp         = sum(mis_2pp),
    prop_2pp        = mis_2pp    / total_loans,
    annual_transfer = sum(annual_transfer)
  )

print(overall_econ_summary)
```

```{r plot_communities_1_4, echo=FALSE, fig.width=12, fig.height=8}
# plot for comparing actual vs predicted interest per community and credit 
# rating 

ggplot(
  all_mispriced_loans %>%
    filter(Credit_Rating %in% c("A", "B", "C"), community %in% c("1", "2", "3", "4")) %>%
    pivot_longer(
      cols = c("Interest", "Predicted_Interest"),
      names_to = "Rate_Type",
      values_to = "Rate_Value"
    ),
  aes(x = Rate_Value, fill = Rate_Type)
) +
  geom_density(alpha = 0.3) +
  facet_grid(community ~ Credit_Rating) +
  labs(
    title = "Actual vs. Predicted Interest Rates for Mispriced Loans\nby Community and Credit Rating",
    x = "Interest Rate (%)",
    y = "Density",
    fill = "Rate Type"
  ) +
  theme_classic(base_size = 11) +
  theme(
    strip.text = element_text(size = 10),
    plot.title = element_text(size = 12, hjust = 0.5, face = "bold"),
    axis.text.x = element_text(size = 9),
    axis.text.y = element_text(size = 9),
    panel.spacing = unit(1, "lines"),
    plot.margin = unit(c(10, 10, 10, 10), "pt"),
    legend.position = "bottom"
  )
```

```{r plot_communities_5_8, echo=FALSE, fig.width=12, fig.height=8}
ggplot(
  all_mispriced_loans %>%
    filter(Credit_Rating %in% c("A", "B", "C"), community %in% c("5", "6", "7")) %>%
    group_by(community, Credit_Rating) %>%
    filter(n() >= 2) %>%
    ungroup() %>%
    pivot_longer(
      cols = c("Interest", "Predicted_Interest"),
      names_to = "Rate_Type",
      values_to = "Rate_Value"
    ),
  aes(x = Rate_Value, fill = Rate_Type)
) +
  geom_density(alpha = 0.3) +
  facet_grid(community ~ Credit_Rating) +
  labs(
    title = "Actual vs. Predicted Interest Rates for Mispriced Loans\nby Community and Credit Rating",
    x = "Interest Rate (%)",
    y = "Density",
    fill = "Rate Type"
  ) +
  theme_classic(base_size = 11) +
  theme(
    strip.text = element_text(size = 10),
    plot.title = element_text(size = 12, hjust = 0.5, face = "bold"),
    axis.text.x = element_text(size = 9),
    axis.text.y = element_text(size = 9),
    panel.spacing = unit(1, "lines"),
    plot.margin = unit(c(10, 10, 10, 10), "pt"),
    legend.position = "bottom"
  ) +
  geom_text(
    data = all_mispriced_loans %>%
      count(community, Credit_Rating) %>%
      filter(n < 2) %>%
      mutate(label_text = "Not enough loans"),
    aes(x = mean(all_mispriced_loans$Interest, na.rm = TRUE), y = 0.02, label = label_text),
    size = 5, color = "red", inherit.aes = FALSE
  )
```

```{r}
# Scatter plot of actual vs. predicted interest rate with model confidence based on weighted
# Eucildean distance of mispriced loan away from the centroid of properly priced loans
# Per community

# ---- STEP 1: Identify Final Varying Features ----

top_features <- rf_reg_result$top_features

# Variation across communities:
varying_features_across <- all_mispriced_loans %>%
  select(community, all_of(top_features)) %>%
  group_by(community) %>%
  summarise(across(everything(), ~ n_distinct(.x) > 1)) %>%
  ungroup() %>%
  summarise(across(-community, any)) %>%
  pivot_longer(cols = everything(), names_to = "Feature", values_to = "Varies") %>%
  filter(Varies) %>%
  pull(Feature)

# Variation within at least one community:
varying_features_within <- all_mispriced_loans %>%
  select(community, all_of(varying_features_across)) %>%
  group_by(community) %>%
  summarise(across(everything(), ~ n_distinct(.x) > 1)) %>%
  ungroup() %>%
  summarise(across(-community, any)) %>%
  pivot_longer(cols = everything(), names_to = "Feature", values_to = "VariesWithin") %>%
  filter(VariesWithin) %>%
  pull(Feature)

# Final features that vary both across and within communities:
top_features_final <- intersect(varying_features_across, varying_features_within)
cat("Final features that vary both across and within communities:\n")
print(top_features_final)

# ---- STEP 2: Compute Normalized Importance Weights ----

importance_scores <- rf_reg_result$rf_importance_df %>%
  filter(Feature %in% top_features_final) %>%
  pull(Importance)
names(importance_scores) <- top_features_final
weights_vector <- importance_scores / sum(importance_scores)

# ---- STEP 3: Compute Centroids for Properly Priced Loans ----

# Adjusted Mode function to retain factor levels if needed
Mode <- function(x) {
  ux <- unique(x)
  mode_val <- ux[which.max(tabulate(match(x, ux)))]
  if (is.factor(x)) {
    mode_val <- factor(mode_val, levels = levels(x))
  }
  return(mode_val)
}

numeric_features <- top_features_final[sapply(df_sample[top_features_final], is.numeric)]
categorical_features <- top_features_final[!sapply(df_sample[top_features_final], is.numeric)]

numeric_centroids <- df_sample %>%
  filter(Mispricing == "Properly Priced") %>%
  group_by(community) %>%
  summarise(across(any_of(numeric_features), ~ mean(.x, na.rm = TRUE))) %>%
  ungroup()

categorical_centroids <- df_sample %>%
  filter(Mispricing == "Properly Priced") %>%
  group_by(community) %>%
  summarise(across(any_of(categorical_features), Mode)) %>%
  ungroup()

properly_priced_centroids <- full_join(numeric_centroids, categorical_centroids, by = "community")
common_predictors <- intersect(top_features_final, colnames(properly_priced_centroids))
cat("Common predictors between top_features_final and centroids:\n")
print(common_predictors)

# ---- New STEP: Identify Binary and Continuous Features ----
# Here we treat one-hot encoded features (only 0 and 1) as binary.
binary_features <- numeric_features[sapply(df_sample[numeric_features], function(x) {
  uniq <- sort(unique(x))
  length(uniq) == 2 && all(uniq %in% c(0, 1))
})]
continuous_features <- setdiff(numeric_features, binary_features)

cat("Binary features:\n")
print(binary_features)
cat("Continuous features:\n")
print(continuous_features)

# ---- New STEP: Precompute Scaling Parameters for Continuous Features ----
scaling_means_cont <- sapply(df_sample[continuous_features], function(x) mean(x, na.rm = TRUE))
scaling_sds_cont   <- sapply(df_sample[continuous_features], function(x) sd(x, na.rm = TRUE))

cat("Scaling means (continuous):\n")
print(scaling_means_cont)
cat("Scaling standard deviations (continuous):\n")
print(scaling_sds_cont)

# ---- New STEP: Define Combined Euclidean Distance Function (Continuous + Binary) ----

compute_euclidean_distance_combined <- function(loan_row, centroid_row, continuous_features, binary_features, weights_vector, scaling_means_cont, scaling_sds_cont, debug = FALSE) {
  # For continuous features: standardize (z-score)
  if (length(continuous_features) > 0) {
    loan_cont <- as.numeric(loan_row[, continuous_features, drop = FALSE])
    centroid_cont <- as.numeric(centroid_row[, continuous_features, drop = FALSE])
    names(loan_cont) <- continuous_features
    names(centroid_cont) <- continuous_features
    loan_cont_z <- (loan_cont - scaling_means_cont[continuous_features]) / scaling_sds_cont[continuous_features]
    centroid_cont_z <- (centroid_cont - scaling_means_cont[continuous_features]) / scaling_sds_cont[continuous_features]
  } else {
    loan_cont_z <- numeric(0)
    centroid_cont_z <- numeric(0)
  }
  
  # For binary features, no scaling is needed (values are 0 or 1)
  if (length(binary_features) > 0) {
    loan_bin <- as.numeric(loan_row[, binary_features, drop = FALSE])
    centroid_bin <- as.numeric(centroid_row[, binary_features, drop = FALSE])
    names(loan_bin) <- binary_features
    names(centroid_bin) <- binary_features
  } else {
    loan_bin <- numeric(0)
    centroid_bin <- numeric(0)
  }
  
  # Get weights for continuous and binary features
  weights_cont <- weights_vector[continuous_features]
  weights_bin  <- weights_vector[binary_features]
  
  # Compute squared differences for continuous features and binary features
  sq_diff_cont <- if (length(loan_cont_z) > 0) weights_cont * (loan_cont_z - centroid_cont_z)^2 else 0
  sq_diff_bin  <- if (length(loan_bin) > 0) weights_bin  * (loan_bin - centroid_bin)^2 else 0
  
  if (debug) {
    cat("Loan continuous z-values:\n"); print(loan_cont_z)
    cat("Centroid continuous z-values:\n"); print(centroid_cont_z)
    cat("Loan binary values:\n"); print(loan_bin)
    cat("Centroid binary values:\n"); print(centroid_bin)
    cat("Weights continuous:\n"); print(weights_cont)
    cat("Weights binary:\n"); print(weights_bin)
  }
  
  total_sq_diff <- sum(sq_diff_cont) + sum(sq_diff_bin)
  distance <- sqrt(total_sq_diff)
  return(distance)
}

# ---- STEP 6: Compute Distances for Each Mispriced Loan & Plot ----

all_mispriced_loans$distance_to_centroid <- NA_real_

for (i in seq_len(nrow(all_mispriced_loans))) {
  community_i <- all_mispriced_loans$community[i]
  loan_row <- all_mispriced_loans[i, , drop = FALSE]
  
  centroid_row <- properly_priced_centroids %>%
    filter(community == community_i) %>%
    select(-community)
  
  if (nrow(centroid_row) == 1) {
    all_mispriced_loans$distance_to_centroid[i] <- compute_euclidean_distance_combined(
      loan_row, centroid_row, continuous_features, binary_features, weights_vector, scaling_means_cont, scaling_sds_cont, debug = FALSE
    )
  } else {
    warning(sprintf("Centroid row issue for community: %s", community_i))
  }
}
```

```{r}
# Compute centroid dates for comparison
properly_priced_centroid_dates <- df_sample %>%
  filter(Mispricing == "Properly Priced") %>%
  group_by(community) %>%
  summarise(
    centroid_date = as.Date(median(as.numeric(date.start)), origin = "1970-01-01"),
    .groups = "drop"
  )

```

```{r}
ggplot(all_mispriced_loans %>% 
         filter(Mispricing_Type %in% c("Underpriced", "Overpriced"),
                community %in% c(1,2)),
       aes(x = Predicted_Interest, y = Interest,
           color = scales::rescale(distance_to_centroid, to = c(0, 1)))) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50") +
  facet_wrap(~ community, scales = "fixed") +
  labs(title = "GBM Prediction Confidence:\nActual vs. Predicted Interest Rates",
       subtitle = "Color indicates rescaled weighted combined Euclidean distance from centroid",
       x = "Predicted Interest Rate (%)",
       y = "Actual Interest Rate (%)",
       color = "Scaled Distance from Centroid") +
  scale_color_viridis_c() +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        legend.position = "right")
```

```{r}
# Filter mispriced loans; here we assume 'Mispricing_Type' flags "Underpriced" or "Overpriced"
# Plot for Communities 3-4 (filtering inline)
ggplot(all_mispriced_loans %>% 
         filter(Mispricing_Type %in% c("Underpriced", "Overpriced"),
                community %in% c(3,4)),
       aes(x = Predicted_Interest, y = Interest,
           color = scales::rescale(distance_to_centroid, to = c(0, 1)))) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50") +
  facet_wrap(~ community, scales = "fixed") +
  labs(title = "GBM Prediction Confidence:\nActual vs. Predicted Interest Rates",
       subtitle = "Color indicates rescaled weighted combined Euclidean distance from centroid",
       x = "Predicted Interest Rate (%)",
       y = "Actual Interest Rate (%)",
       color = "Scaled Distance from Centroid") +
  scale_color_viridis_c() +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        legend.position = "right")
```

```{r}
# Plot for Communities 5-7 (filtering inline)
ggplot(all_mispriced_loans %>% 
         filter(Mispricing_Type %in% c("Underpriced", "Overpriced"),
                community %in% c(5,6,7)),
       aes(x = Predicted_Interest, y = Interest,
           color = scales::rescale(distance_to_centroid, to = c(0, 1)))) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50") +
  facet_wrap(~ community, scales = "fixed") +
  labs(title = "GBM Prediction Confidence:\nActual vs. Predicted Interest Rates",
       subtitle = "Color indicates rescaled weighted combined Euclidean distance from centroid",
       x = "Predicted Interest Rate (%)",
       y = "Actual Interest Rate (%)",
       color = "Scaled Distance from Centroid") +
  scale_color_viridis_c() +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        legend.position = "right")
```

```{r}
# all communities in one plot

ggplot(all_mispriced_loans %>% 
         filter(Mispricing_Type %in% c("Underpriced", "Overpriced"),
                community %in% c(1,2,3,4,5,6,7)),
       aes(x = Predicted_Interest, y = Interest,
           color = scales::rescale(distance_to_centroid, to = c(0, 1)))) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50") +
  facet_wrap(~ community, scales = "fixed") +
  labs(title = "GBM Prediction Confidence:\nActual vs. Predicted Interest Rates",
       subtitle = "Color indicates rescaled weighted combined Euclidean distance from centroid",
       x = "Predicted Interest Rate (%)",
       y = "Actual Interest Rate (%)",
       color = "Scaled Distance from Centroid") +
  scale_color_viridis_c() +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        legend.position = "right")
```

```{r}
# Scatter plot residuals vs distance to centroid with time component as color gradient

ggplot(
  all_mispriced_loans %>%
    filter(Mispricing_Type %in% c("Underpriced", "Overpriced")) %>%
    mutate(Prediction_Error = Interest - Predicted_Interest),
  aes(
    x = Prediction_Error,
    y = distance_to_centroid,
    color = as.numeric(date.start)
  )
) +
  geom_point(alpha = 0.7, size = 2) +
  facet_wrap(~ community, scales = "fixed") +
  labs(
    title = "Interest Rate Differences vs. Distance to Centroid",
    subtitle = "Color indicates loan issuance (start) date (earlier -> darker, later -> brighter)",
    x = "Interest Rate Difference (Actual - Predicted)",
    y = "Distance to Centroid",
    color = "Issuance Date"
  ) +
  scale_color_viridis_c(
    # Pick suitable breaks for your data's date range:
    breaks = as.numeric(as.Date(c("2012-01-01", "2014-01-01", 
                                  "2016-01-01", "2018-01-01",
                                  "2020-01-01", "2022-01-01"))),
    # Format each break as YYYY:
    labels = c("2012", "2014", "2016", "2018", "2020", "2022")
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    legend.position = "right"
  )
```

## Analyzing the extreme tails of the interest rate differences

### Testing the significance of Overpricing and Underpricing Within Clusters

```{r}
# Group data by community and Mispricing_Type
mispricing_stats <- all_mispriced_loans %>%
  group_by(community, Mispricing_Type) %>%
  summarise(
    Mean_Interest_Difference = mean(Interest_Difference),
    SD_Interest_Difference = sd(Interest_Difference),
    N = n()
  )

# Perform t-test
t_test_results <- mispricing_stats %>%
  rowwise() %>%
  mutate(
    t_statistic = Mean_Interest_Difference / (SD_Interest_Difference / sqrt(N)),
    p_value = 2 * pt(-abs(t_statistic), df = N - 1)
  )

print(t_test_results)
```

```{r}
# Check overall loan duration per community in all_mispriced_loans
# For each community, count how many loans have a given duration set to 1
mispriced_duration_summary <- all_mispriced_loans %>%
  group_by(community) %>%
  summarize(
    across(
      all_of(duration_cols),
      ~ sum(. == 1, na.rm = TRUE),     # Sums the rows where this duration column equals 1
      .names = "count_{.col}"         # e.g. "count_duration.06"
    )
  )

# View the result
print(mispriced_duration_summary)
```

```{r}
properly_duration_summary <- properly_priced_data %>%
  group_by(community) %>%
  summarize(
    across(
      all_of(duration_cols),
      ~ sum(. == 1, na.rm = TRUE),
      .names = "count_{.col}"
    )
  )

print(properly_duration_summary)
```

## Robustness check GBM

### Time-Split Validation with Rolling Windows

#### Window 1: 2012-01-01 until 2016-12-31, Test: 2017-01-01 until 2017-12-31

#### Window 2: 2017-01-01 until 2020-12-31, Test: 2021-01-01 until 2021-12-31

```{r}
##########################################################
# SLIDING WINDOW EXAMPLE
# Windows:
#   1) Train on 2012-2016, Test on 2017
#   2) Train on 2017-2020, Test on 2021
##########################################################

# 1) Define your windows in a list:
windows <- list(
  list(
    name         = "Window1",
    train_start  = as.Date("2012-01-01"),
    train_end    = as.Date("2016-12-31"),
    test_start   = as.Date("2017-01-01"),
    test_end     = as.Date("2017-12-31")
  ),
  list(
    name         = "Window2",
    train_start  = as.Date("2017-01-01"),
    train_end    = as.Date("2020-12-31"),
    test_start   = as.Date("2021-01-01"),
    test_end     = as.Date("2021-12-31")
  )
)

# 2) (Optional) Setup your caretaker trainControl, narrower GBM grid, etc.
#    Adjust these to handle smaller data sets.
train_ctrl <- trainControl(method = "cv", number = 3)

#gbm_grid <- expand.grid(
#  interaction.depth = c(3),      # keep it small
#  n.trees           = c(50, 100),# fewer trees
#  shrinkage         = c(0.1),    # single rate
#  n.minobsinnode    = c(2, 5)    # small node sizes
#)

gbm_grid <- expand.grid(
  interaction.depth = c(3, 5),      # Tree depth
  n.trees           = c(100, 200),  # Number of boosting iterations
  shrinkage         = c(0.1, 0.3),  # Learning rate
  n.minobsinnode    = c(10, 20)     # Min # of obs in terminal node
)

# 3) Initialize storage
window_models           <- list()
window_results          <- list()
window_mispricing_summ  <- list()

# 4) Loop over windows, then communities
for (w in seq_along(windows)) {
  
  winfo <- windows[[w]]
  cat("\n============================================\n")
  cat("  PROCESSING", winfo$name, 
      "   Train:", winfo$train_start, "to", winfo$train_end,
      "   Test:",  winfo$test_start,  "to", winfo$test_end, "\n")
  cat("============================================\n")
  
  # Use a sub-list or environment to store results for this window
  community_models          <- list()
  community_results         <- list()
  mispricing_summary_list   <- list()
  
  for (comm in unique(df_sample$community)) {
    cat("\nCommunity:", comm, "(", winfo$name, ")...\n")
    
    # Subset data for the community
    df_comm <- df_sample %>% 
      filter(community == comm)
    
    # Subset "properly priced" only for training
    properly_priced_comm <- df_comm %>%
      filter(Mispricing == "Properly Priced")
    
    # Training range: (train_start <= date.start <= train_end)
    train_data <- properly_priced_comm %>%
      filter(date.start >= winfo$train_start,
             date.start <= winfo$train_end)
    
    # Test range: (test_start <= date.start <= test_end)
    test_data <- properly_priced_comm %>%
      filter(date.start >= winfo$test_start,
             date.start <= winfo$test_end)
    
    # If not enough data, skip
    if (nrow(train_data) < 10 || nrow(test_data) < 10) {
      cat("Skipping: not enough train/test data for Community", comm, "\n")
      next
    }
    
    #----------------------------------------------------
    # Remove single-level variables in the train set
    #----------------------------------------------------
    unique_value_counts <- sapply(
      train_data[, predictor_vars, drop = FALSE],
      function(x) length(unique(x))
    )
    single_level_vars <- names(unique_value_counts[unique_value_counts <= 1])
    
    if (length(single_level_vars) > 0) {
      cat("Single-level vars (train) for Community", comm, ":", 
          paste(single_level_vars, collapse=", "), "\n")
    }
    
    predictor_vars_comm <- setdiff(predictor_vars, single_level_vars)
    if (length(predictor_vars_comm) == 0) {
      cat("No valid predictors left. Skipping.\n")
      next
    }
    
    # Prepare train/test subsets
    train_data_sub <- train_data[, c(predictor_vars_comm, "Interest"), drop=FALSE]
    test_data_sub  <- test_data[, c(predictor_vars_comm, "Interest"), drop=FALSE]
    
    #----------------------------------------------------
    # Fit the GBM on the training set
    #----------------------------------------------------
    formula_str <- paste("Interest", "~", paste(predictor_vars_comm, collapse=" + "))
    cat("Caret GBM formula for Comm", comm, "(", winfo$name, "):", formula_str, "\n")
    
    set.seed(123)
    gbm_model_caret <- train(
      form        = as.formula(formula_str),
      data        = train_data_sub,
      method      = "gbm",
      trControl   = train_ctrl,
      tuneGrid    = gbm_grid,
      metric      = "RMSE",
      verbose     = FALSE,
      preProcess  = "zv",
      bag.fraction= 0.8
    )
    
    # Best model
    best_gbm_model <- gbm_model_caret$finalModel
    
    #----------------------------------------------------
    # Evaluate on the test set
    #----------------------------------------------------
    test_preds <- predict(gbm_model_caret, newdata=test_data_sub)
    
    rmse_val <- sqrt(mean((test_data_sub$Interest - test_preds)^2))
    mae_val  <- mean(abs(test_data_sub$Interest - test_preds))
    
    ss_res <- sum((test_data_sub$Interest - test_preds)^2)
    ss_tot <- sum((test_data_sub$Interest - mean(test_data_sub$Interest))^2)
    r_squared_val <- 1 - ss_res/ss_tot
    
    cat("Test RMSE:", rmse_val, "MAE:", mae_val, 
        "R-sq:", r_squared_val, "\nBestTune:\n")
    print(gbm_model_caret$bestTune)
    
    #----------------------------------------------------
    # (Optional) Predict thresholds using *all*
    # properly priced data from this community's 
    # training window (train_data or the combined if you prefer).
    #----------------------------------------------------
    # We'll define "train_data" as the basis for threshold 
    # or we can combine train_data + test_data if you prefer.
    
    # For demonstration: 
    #   threshold is computed from the entire "properly priced" 
    #   within the *training window* only
    #   to reflect ex-ante knowledge
    train_preds <- predict(gbm_model_caret, newdata=train_data_sub)
    train_data$Predicted_Interest <- train_preds
    train_data$Interest_Difference <- train_preds - train_data$Interest
    
    pos_diff <- train_data$Interest_Difference[train_data$Interest_Difference > 0]
    threshold_underpriced <- mean(pos_diff, na.rm=TRUE) + sd(pos_diff, na.rm=TRUE)
    
    neg_diff <- train_data$Interest_Difference[train_data$Interest_Difference < 0]
    threshold_overpriced <- mean(neg_diff, na.rm=TRUE) - sd(neg_diff, na.rm=TRUE)
    
    #----------------------------------------------------
    # (Optional) Predict on mispriced loans in 
    # the *test window* if you want to see how they'd 
    # be labeled ex-ante
    #----------------------------------------------------
    mispriced_test_data <- df_comm %>%
      filter(Mispricing %in% c("Overpriced","Underpriced")) %>%
      filter(date.start >= winfo$test_start, 
             date.start <= winfo$test_end)
    
    if (nrow(mispriced_test_data) > 0) {
      
      # remove any single-level vars not in training
      mispriced_test_subset <- mispriced_test_data[, 
          c(predictor_vars_comm, "Interest"), drop=FALSE]
      
      test_mis_preds <- predict(gbm_model_caret, newdata=mispriced_test_subset)
      mispriced_test_data$Predicted_Interest <- test_mis_preds
      mispriced_test_data$Interest_Difference <- test_mis_preds - mispriced_test_data$Interest
      
      mispriced_test_data <- mispriced_test_data %>%
        mutate(Mispricing_Type = case_when(
          Interest_Difference > threshold_underpriced ~ "Underpriced",
          Interest_Difference < threshold_overpriced  ~ "Overpriced",
          TRUE ~ "Properly Priced"
        ))
      
      # Store results
      store_key <- paste(winfo$name, "Comm", comm, sep="_")
      community_results[[store_key]] <- mispriced_test_data
      
      # Summarize
      mispricing_summary <- mispriced_test_data %>%
        group_by(Mispricing_Type) %>%
        summarise(
          Count                    = n(),
          Mean_Actual_Interest     = mean(Interest, na.rm=TRUE),
          Mean_Predicted_Interest  = mean(Predicted_Interest, na.rm=TRUE),
          Mean_Interest_Difference = mean(Interest_Difference, na.rm=TRUE),
          .groups = "drop"
        ) %>%
        mutate(Community=comm, Window=winfo$name)
      
      mispricing_summary_list[[store_key]] <- mispricing_summary
    } else {
      cat("No mispriced test loans to predict in community", comm, "(", winfo$name, ")\n")
    }
    
    # Save caretaker model
    model_key <- paste(winfo$name, "Comm", comm, sep="_")
    community_models[[model_key]] <- gbm_model_caret
  } # end loop over communities
  
  # Save or combine final results from this window
  window_models[[winfo$name]]          <- community_models
  window_results[[winfo$name]]         <- community_results
  window_mispricing_summ[[winfo$name]] <- mispricing_summary_list
}

cat("\nSliding Window Approach Complete.\n")
```
